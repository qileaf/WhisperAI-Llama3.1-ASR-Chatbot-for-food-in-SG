{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WhisperAI Fine-tuning for location names in Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\faeli\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperTokenizer\n",
    "from datasets import Dataset, DatasetDict, load_dataset, ClassLabel, Features, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n",
      "GPU name: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Device used:', device)\n",
    "print(f\"GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Use Case that the model does not recognize Singapore locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .wav file and downsample the audio to 16000Hz as required by WhisperAI\n",
    "file_path = \"Location_Trial.wav\"\n",
    "audio_data, sampling_rate = librosa.load(file_path, sr=16000)  # sr=16000 to convert the sampling rate to 16000Hz\n",
    "\n",
    "# Instantiante the processor and model from HuggingFace Hub\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base.en\")\n",
    "\n",
    "# Use the model and processor to transcribe the audio:\n",
    "input_features = processor(\n",
    "    audio_data, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "# Generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "\n",
    "# Decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I have a friend staying at Songye Kadut. We love to go to Gailan to eat and we have family staying in Telot Blanga.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 648/648 [00:06<00:00, 97.45 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Function to load audio and metadata\n",
    "def load_audio(data):\n",
    "    audio_path = data['audio_path']\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    return {'audio': audio, 'text': data['transcription']}\n",
    "\n",
    "# Load metadata into a DataFrame\n",
    "metadata_df = pd.read_csv('metadata.csv')\n",
    "\n",
    "# Define features for the dataset\n",
    "features = Features({\n",
    "    'audio': Value('float32'),\n",
    "    'transcription': Value('string'),\n",
    "})\n",
    "\n",
    "# Load audio files into dataset\n",
    "train_set = Dataset.from_pandas(metadata_df)\n",
    "\n",
    "# Preprocess audio data\n",
    "train_set = train_set.map(load_audio, remove_columns=['audio_path', 'transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'text'],\n",
       "    num_rows: 648\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the process to create the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 162/162 [00:00<00:00, 482.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load metadata into a DataFrame\n",
    "test_metadata_df = pd.read_csv('metadata_test.csv')\n",
    "\n",
    "# Define features for the dataset\n",
    "features = Features({\n",
    "    'audio': Value('float32'),\n",
    "    'transcription': Value('string'),\n",
    "})\n",
    "\n",
    "# Load audio files into dataset\n",
    "test_set = Dataset.from_pandas(test_metadata_df)\n",
    "\n",
    "# Preprocess audio data\n",
    "test_set = test_set.map(load_audio, remove_columns=['audio_path', 'transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'text'],\n",
       "    num_rows: 162\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DatasetDict with both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 648\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 162\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_dataset = DatasetDict({\n",
    "        'train': train_set,\n",
    "        'test': test_set,\n",
    "})\n",
    "\n",
    "whisper_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning WhisperAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model and processor\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-base.en')\n",
    "processor = WhisperProcessor.from_pretrained('openai/whisper-base.en')\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-base.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 648/648 [01:02<00:00, 10.32 examples/s]\n",
      "Map: 100%|██████████| 162/162 [00:05<00:00, 32.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(data):\n",
    "    # Process audio\n",
    "    audio = data[\"audio\"]\n",
    "\n",
    "    # Get the processed audio input values\n",
    "    processed_audio = processor(audio, sampling_rate=16000)\n",
    "\n",
    "    # Get the input features\n",
    "    input_features = processed_audio[\"input_features\"]\n",
    "\n",
    "    # Convert input_features to a tensor\n",
    "    data[\"input_features\"] = torch.tensor(input_features[0], dtype=torch.float32)\n",
    "\n",
    "    # Process text (tokenize)\n",
    "    labels = tokenizer(\n",
    "        data[\"text\"], return_tensors=\"pt\", padding=\"max_length\", max_length=128, truncation=True\n",
    "    ).input_ids\n",
    "\n",
    "    # Convert labels to a tensor (1D)\n",
    "    data[\"labels\"] = labels[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply the preprocessing function to the datasets\n",
    "whisper_dataset = whisper_dataset.map(preprocess_function, remove_columns=['audio', 'text'])\n",
    "whisper_dataset.set_format(type=\"torch\", columns=[\"input_features\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[ 0.7029,  0.6257,  0.4525,  ..., -0.7932, -0.7932, -0.7932],\n",
       "         [ 0.4301,  0.3983,  0.2610,  ..., -0.7932, -0.7932, -0.7932],\n",
       "         [ 0.2831,  0.1739,  0.1047,  ..., -0.7932, -0.7932, -0.7932],\n",
       "         ...,\n",
       "         [-0.4738, -0.7932, -0.7357,  ..., -0.7932, -0.7932, -0.7932],\n",
       "         [-0.4476, -0.7013, -0.6744,  ..., -0.7932, -0.7932, -0.7932],\n",
       "         [-0.3572, -0.7842, -0.7932,  ..., -0.7932, -0.7932, -0.7932]]),\n",
       " 'labels': tensor([50257, 50362,    40,   423,   257,  1256,   286,  2460, 10589,   287,\n",
       "          2895,  4270,   509,   952,    13,  3574,   616,  2156,    11,  2895,\n",
       "          4270,   509,   952,   318,   257,  1310,  1290,    13,   317,  4451,\n",
       "          1545,   286,  6164,   468,   257, 16394,   290, 14768,   287,  2895,\n",
       "          4270,   509,   952,    13,  6674,   314,   815,  1445,   284,  2895,\n",
       "          4270,   509,   952,    13,  3914,   338,   467,   284,  2895,  4270,\n",
       "           509,   952,     0, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check to ensure the data has been preprocessed to the right format\n",
    "whisper_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an evaluation metric called \"Word Error Rate\" or WER in short. This is a commonly used metric to evaluate the performance of automatic speech recognition systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    predictions = pred.predictions.argmax(-1)\\n    decoded_predictions = processor.batch_decode(predictions, skip_special_tokens=True)\\n    decoded_labels = processor.batch_decode(pred.label_ids, skip_special_tokens=True)\\n\\n    # Update WER metric\\n    wer_score = eval_metric.compute(predictions=decoded_predictions, references=decoded_labels)\\n    return {\"wer\": wer_score[\"wer\"]}\\n    \\n    pred_ids = eval_pred.predictions\\n    label_ids = eval_pred.label_ids\\n\\n    pred_decoded = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\\n    label_decoded = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\\n\\n    wer = 100 * eval_metric.compute(predictions=pred_decoded, references=label_decoded)\\n\\n    return {\\'WER\\': wer}    '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metric = evaluate.load('wer')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    # Extract logits and labels from the eval_pred tuple\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Get the predicted indices\n",
    "    predictions = logits[0].argmax(axis=-1)\n",
    "\n",
    "    # Decode predictions and references\n",
    "    decoded_predictions = processor.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Update WER metric\n",
    "    wer_score = eval_metric.compute(predictions=decoded_predictions, references=decoded_labels)\n",
    "    return {\"wer\": wer_score}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51864, 512, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=512, out_features=51864, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Collator\n",
    "def data_collator(batch):\n",
    "    input_values = torch.stack([f[\"input_features\"] for f in batch])\n",
    "    labels = torch.stack([f[\"labels\"] for f in batch])\n",
    "    return {\"input_features\": input_values, \"labels\": labels}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',              # output directory\n",
    "    evaluation_strategy=\"epoch\",         # evaluation strategy\n",
    "    save_strategy=\"epoch\",              # save checkpoint after every epoch\n",
    "    learning_rate=2e-5,                  # learning rate\n",
    "    per_device_train_batch_size=8,      # batch size for training\n",
    "    per_device_eval_batch_size=2,       # batch size for evaluation\n",
    "    num_train_epochs=5,                  # number of training epochs\n",
    "    weight_decay=0.01,                   # strength of weight decay\n",
    "    logging_dir='./logs',                # directory for storing logs\n",
    "    fp16=True,                           # enable mixed precision to reduce memory usage\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                                            # the model to be trained\n",
    "    args=training_args,                                     # training arguments\n",
    "    data_collator=data_collator,                            # data collator\n",
    "    train_dataset=whisper_dataset['train'],                 # training dataset\n",
    "    eval_dataset=whisper_dataset['test'],                   # evaluation dataset (optional)\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 81/405 [00:49<00:39,  8.18it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.016931457445025444, 'eval_wer': 0.06804478897502153, 'eval_runtime': 38.9152, 'eval_samples_per_second': 4.163, 'eval_steps_per_second': 2.081, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 40%|████      | 162/405 [01:25<00:29,  8.22it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.015643645077943802, 'eval_wer': 0.06632213608957795, 'eval_runtime': 20.1312, 'eval_samples_per_second': 8.047, 'eval_steps_per_second': 4.024, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 243/405 [03:12<02:24,  1.12it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014437884092330933, 'eval_wer': 0.06287683031869079, 'eval_runtime': 32.6822, 'eval_samples_per_second': 4.957, 'eval_steps_per_second': 2.478, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 324/405 [04:48<01:10,  1.15it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014445878565311432, 'eval_wer': 0.06287683031869079, 'eval_runtime': 23.7332, 'eval_samples_per_second': 6.826, 'eval_steps_per_second': 3.413, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405/405 [05:45<00:00,  1.46it/s]Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n",
      "                                                 \n",
      "100%|██████████| 405/405 [06:17<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014320522546768188, 'eval_wer': 0.06201550387596899, 'eval_runtime': 30.6139, 'eval_samples_per_second': 5.292, 'eval_steps_per_second': 2.646, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361], 'begin_suppress_tokens': [220, 50256]}\n",
      "100%|██████████| 405/405 [06:18<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 378.4499, 'train_samples_per_second': 8.561, 'train_steps_per_second': 1.07, 'train_loss': 0.022598238344545718, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=405, training_loss=0.022598238344545718, metrics={'train_runtime': 378.4499, 'train_samples_per_second': 8.561, 'train_steps_per_second': 1.07, 'total_flos': 2.101463875584e+17, 'train_loss': 0.022598238344545718, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start fine-tuning the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually saving the processor after the fine-tuning has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a checkpoint from the fine-tuning to be used as the model for prediction. From the above fine-tuning results, the last checkpoint (checkpoint-405) gave the best wer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"results/checkpoint-405\"\n",
    "\n",
    "# Load the model and processor from the checkpoint\n",
    "model = WhisperForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "processor = WhisperProcessor.from_pretrained(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the first file we trialed on the original Whisper-base.en model, after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a friend staying at Sungei Kadut. We love to go to Geylang to eat. And we have family staying in Telok Blangah.\n"
     ]
    }
   ],
   "source": [
    "# Moving model to CUDA\n",
    "model.to(device)\n",
    "\n",
    "# Read the .wav file and downsample the audio to 16000Hz as required by WhisperAI\n",
    "file_path = \"Location_Trial.wav\"\n",
    "audio_data, sampling_rate = librosa.load(file_path, sr=16000)  # sr=16000 to convert the sampling rate to 16000Hz\n",
    "\n",
    "# Use the model and processor to transcribe the audio:\n",
    "input_features = processor(\n",
    "    audio_data, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "# Move input features to the same device as the model\n",
    "input_features = input_features.to(device)\n",
    "\n",
    "# Generate token ids using the model\n",
    "with torch.no_grad():  # Disable gradient tracking for inference\n",
    "    predicted_ids = model.generate(input_features)\n",
    "\n",
    "# Decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print the transcription\n",
    "print(transcription[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
